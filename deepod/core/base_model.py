# -*- coding: utf-8 -*-
"""
Base class for deep Anomaly detection models
some functions are adapted from the pyod library
@Author: Hongzuo Xu <hongzuoxu@126.com, xuhongzuo13@nudt.edu.cn>
"""

import numpy as np
import pandas as pd
import torch
import random
import time
from abc import ABCMeta, abstractmethod
from scipy.stats import binom
from deepod.utils.utility import get_sub_seqs, get_sub_seqs_label, get_sub_seqs_label2
from deepod.core.networks.base_networks import sequential_net_name
from tqdm import tqdm
from torch.utils.data import DataLoader
from scipy import stats
from torch.autograd import grad
from torch.autograd.functional import hessian
from testbed.utils import split
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score
from scipy.spatial.distance import pdist
from sklearn.preprocessing import normalize
import torch.nn.functional as F


class BaseDeepAD(metaclass=ABCMeta):
    """
    Abstract class for deep outlier detection models

    Parameters
    ----------

    data_type: str, optional (default='tabular')
        Data type, choice = ['tabular', 'ts']

    network: str, optional (default='MLP')
        network structure for different data structures

    epochs: int, optional (default=100)
        Number of training epochs

    batch_size: int, optional (default=64)
        Number of samples in a mini-batch

    lr: float, optional (default=1e-3)
        Learning rate

    n_ensemble: int or str, optional (default=1)
        Number of ensemble size

    seq_len: int, optional (default=100)
        Size of window used to create subsequences from the data
        deprecated when handling tabular data (network=='MLP')

    stride: int, optional (default=1)
        number of time points the window will move between two subsequences
        deprecated when handling tabular data (network=='MLP')

    epoch_steps: int, optional (default=-1)
        Maximum steps in an epoch
            - If -1, all the batches will be processed

    prt_steps: int, optional (default=10)
        Number of epoch intervals per printing

    device: str, optional (default='cuda')
        torch device,

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    verbose: int, optional (default=1)
        Verbosity mode

    random_state： int, optional (default=42)
        the seed used by the random

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    """
    def __init__(self, model_name, data_type='tabular', network='MLP',
                 epochs=100, batch_size=64, lr=1e-3,
                 n_ensemble=1, seq_len=100, stride=1,
                 epoch_steps=-1, prt_steps=10,
                 device='cuda', contamination=0.1,
                 verbose=1, random_state=42, sample_selection=0):
        self.model_name = model_name

        self.data_type = data_type
        self.network = network

        # if data_type == 'ts':
        #     assert self.network in sequential_net_name, \
        #         'Assigned network cannot handle time-series data'

        self.seq_len = seq_len
        self.stride = stride

        self.epochs = epochs
        self.batch_size = batch_size
        self.lr = lr

        self.device = device
        self.contamination = contamination

        self.epoch_steps = epoch_steps
        self.prt_steps = prt_steps
        self.verbose = verbose

        self.n_features = -1
        self.n_samples = -1
        self.criterion = None
        self.net = None

        self.n_ensemble = n_ensemble

        self.train_loader = None
        self.test_loader = None

        self.epoch_time = None

        self.train_data = None
        self.train_label = None

        self.decision_scores_ = None
        self.labels_ = None
        self.threshold_ = None

        self.random_state = random_state
        self.set_seed(random_state)

        self.sample_selection = sample_selection
        self.save_rate = 0.8
        self.add_rate = 0.5
        self.del_rate = 0.1
        self.train_loss_now = None
        self.train_loss_past = None
        self.thresh = 4e-6
        self.ori_data = None
        self.ori_label = None
        self.seq_starts = None
        self.split = split(self.seq_len)
        # self.pca = PCA(n_components=32)
        self.true_key_param = None
        self.param_musk = None
        self.params = []
        self.all_v = None
        self.trainsets = {}
        self.result_detail = []
        self.g_detail = {}
        return

    def fit(self, X, y=None):
        """
        Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : numpy array of shape (n_samples, )
            Not used in unsupervised methods, present for API consistency by convention.
            used in (semi-/weakly-) supervised methods

        Returns
        -------
        self : object
            Fitted estimator.
        """

        if self.data_type == 'ts':
            X_seqs = get_sub_seqs(X, seq_len=self.seq_len, stride=self.stride)
            y_seqs = get_sub_seqs_label(y, seq_len=self.seq_len, stride=self.stride) if y is not None else None
            self.train_data = X_seqs
            self.train_label = y_seqs
            self.n_samples, self.n_features = X_seqs.shape[0], X_seqs.shape[2]
        else:
            self.train_data = X
            self.train_label = y
            self.n_samples, self.n_features = X.shape

        if self.verbose >= 1:
            print('Start Training...')

        if self.n_ensemble == 'auto':
            self.n_ensemble = int(np.floor(100 / (np.log(self.n_samples) + self.n_features)) + 1)
        if self.verbose >= 1:
            print(f'ensemble size: {self.n_ensemble}')

        for _ in range(self.n_ensemble):
            self.train_loader, self.net, self.criterion = self.training_prepare(self.train_data,
                                                                                y=self.train_label)
            self._training()

        if self.verbose >= 1:
            print('Start Inference on the training data...')

        self.decision_scores_ = self.decision_function(X)
        self.labels_ = self._process_decision_scores()

        return self

    def decision_function(self, X, return_rep=False):
        """Predict raw anomaly scores of X using the fitted detector.

        The anomaly score of an input sample is computed based on the fitted
        detector. For consistency, outliers are assigned with
        higher anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        return_rep: boolean, optional, default=False
            whether return representations

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        testing_n_samples = X.shape[0]

        if self.data_type == 'ts':
            X = get_sub_seqs(X, seq_len=self.seq_len, stride=1)

        representations = []
        s_final = np.zeros(testing_n_samples)
        for _ in range(self.n_ensemble):
            self.test_loader = self.inference_prepare(X)

            z, scores = self._inference()
            z, scores = self.decision_function_update(z, scores)

            if self.data_type == 'ts':
                padding = np.zeros(self.seq_len-1)
                scores = np.hstack((padding, scores))

            s_final += scores
            representations.extend(z)
        representations = np.array(representations)

        if return_rep:
            return s_final, representations
        else:
            return s_final

    def predict(self, X, return_confidence=False):
        """Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        return_confidence : boolean, optional(default=False)
            If True, also return the confidence of prediction.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.
        confidence : numpy array of shape (n_samples,).
            Only if return_confidence is set to True.
        """

        pred_score = self.decision_function(X)
        prediction = (pred_score > self.threshold_).astype('int').ravel()

        if return_confidence:
            confidence = self._predict_confidence(pred_score)
            return prediction, confidence

        return prediction

    def _predict_confidence(self, test_scores):
        """Predict the model's confidence in making the same prediction
        under slightly different training sets.
        See :cite:`perini2020quantifying`.

        Parameters
        -------
        test_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.

        Returns
        -------
        confidence : numpy array of shape (n_samples,)
            For each observation, tells how consistently the model would
            make the same prediction if the training set was perturbed.
            Return a probability, ranging in [0,1].

        """
        n = len(self.decision_scores_)

        count_instances = np.vectorize(lambda x: np.count_nonzero(self.decision_scores_ <= x))
        n_instances = count_instances(test_scores)

        # Derive the outlier probability using Bayesian approach
        posterior_prob = np.vectorize(lambda x: (1 + x) / (2 + n))(n_instances)

        # Transform the outlier probability into a confidence value
        confidence = np.vectorize(
            lambda p: 1 - binom.cdf(n - int(n*self.contamination), n, p)
        )(posterior_prob)
        prediction = (test_scores > self.threshold_).astype('int').ravel()
        np.place(confidence, prediction==0, 1-confidence[prediction == 0])
        return confidence

    def _process_decision_scores(self):
        """Internal function to calculate key attributes:

        - threshold_: used to decide the binary label
        - labels_: binary labels of training data

        Returns
        -------
        self
        """

        self.threshold_ = np.percentile(self.decision_scores_, 100 * (1 - self.contamination))
        self.labels_ = (self.decision_scores_ > self.threshold_).astype('int').ravel()

        self._mu = np.mean(self.decision_scores_)
        self._sigma = np.std(self.decision_scores_)

        return self

    def _training(self):
        optimizer = torch.optim.Adam(self.net.parameters(),
                                     lr=self.lr,
                                     eps=1e-6)

        self.net.train()
        for i in range(self.epochs):
            t1 = time.time()
            total_loss = 0
            cnt = 0
            for batch_x in self.train_loader:
                loss = self.training_forward(batch_x, self.net, self.criterion)
                self.net.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                cnt += 1

                # terminate this epoch when reaching assigned maximum steps per epoch
                if cnt > self.epoch_steps != -1:
                    break

            t = time.time() - t1
            if self.verbose >= 1 and (i == 0 or (i+1) % self.prt_steps == 0):
                print(f'epoch{i+1:3d}, '
                      f'training loss: {total_loss/cnt:.6f}, '
                      f'time: {t:.1f}s')

            if i == 0:
                self.epoch_time = t

            self.epoch_update()

        return

    def _inference(self):
        self.net.eval()
        with torch.no_grad():
            z_lst = []
            score_lst = []

            if self.verbose >= 2:
                _iter_ = tqdm(self.test_loader, desc='testing: ')
            else:
                _iter_ = self.test_loader

            for batch_x in _iter_:
                batch_z, s = self.inference_forward(batch_x, self.net, self.criterion)
                z_lst.append(batch_z)
                score_lst.append(s)

        z = torch.cat(z_lst).data.cpu().numpy()
        scores = torch.cat(score_lst).data.cpu().numpy()

        return z, scores

    @abstractmethod
    def training_forward(self, batch_x, net, criterion):
        """define forward step in training"""
        pass

    @abstractmethod
    def inference_forward(self, batch_x, net, criterion):
        """define forward step in inference"""
        pass

    @abstractmethod
    def training_prepare(self, X, y):
        """define train_loader, net, and criterion"""
        pass

    @abstractmethod
    def inference_prepare(self, X):
        """define test_loader"""
        pass

    def epoch_update(self):
        """for any updating operation after each training epoch"""
        return

    def decision_function_update(self, z, scores):
        """for any updating operation after decision function"""
        return z, scores

    @staticmethod
    def set_seed(seed):
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)
        # torch.backends.cudnn.benchmark = False
        # torch.backends.cudnn.deterministic = True

    def do_sample_selection(self, epoch):
        if self.sample_selection == 0:          # 无操作
            self.net.eval()  # 使用完全的网络来计算
            train_loss_now = np.array([])
            for ii, batch_x in enumerate(self.train_loader):
                _, losses = self.inference_forward(batch_x, self.net, self.criterion)
                train_loss_now = np.concatenate([train_loss_now, losses.cpu().detach().numpy()])
            self.trainsets['loss' + str(epoch)] = train_loss_now
            self.net.train()  # 使用完全的网络来计算

        elif self.sample_selection == 5:        # ICLR21
            pass

        elif self.sample_selection == 6:        # arxiv22
            if len(self.train_data) <= int(self.n_samples*0.3):
                return

            # 计算损失值
            self.net.eval()                     # 使用完全的网络来计算
            train_loss_now = np.array([])
            for batch_x in self.train_loader:
                _, error = self.inference_forward(batch_x, self.net, self.criterion)
                train_loss_now = np.concatenate([train_loss_now, error.cpu().detach().numpy()])
            self.net.train()  # 使用完全的网络来计算
            self.train_loss_now = train_loss_now

            if self.train_loss_past is None:    # 第一轮迭代，直接返回
                self.train_loss_past = self.train_loss_now
                return

            save_num = max(int(self.save_rate * len(self.train_data)), int(self.n_samples*0.3))
            if epoch == 0:
                index1 = np.array([])
            else:
                delta = abs(self.train_loss_now - self.train_loss_past)
                index1 = delta.argsort()[:save_num]
            index2 = train_loss_now.argsort()[:save_num]

            index = np.concatenate([index1, index2])
            index = np.sort(index)
            index = np.unique(index, axis=0)

            self.train_data = self.train_data[np.sort(index)]
            self.train_loss_past = self.train_loss_now[np.sort(index)]
            self.train_loader = DataLoader(self.train_data, batch_size=self.batch_size, drop_last=False,
                                      shuffle=True, pin_memory=True)

            self.trainsets['dis' + str(epoch)] = train_loss_now
            if epoch > 0 and self.train_label is not None:
                self.trainsets['yseq' + str(epoch)] = self.trainsets['yseq' + str(epoch-1)][np.sort(index)]

        elif self.sample_selection == 7:        # 我的方法
            if epoch >= 10:
                return

            # dis = np.zeros(len(self.train_data))
            importance = None
            metrics = np.array([])
            self.net.eval()
            for ii, batch_x in enumerate(self.train_loader):
                # metric = self.get_importance_dL(batch_x, epoch, ii)
                metric = self.get_importance_ICLR21(batch_x, epoch, ii)
                # metric = self.get_importance_ICML17(batch_x, epoch, ii)       # 巨慢

                if epoch == 0:
                    if ii == 0:
                        importance = np.sum(metric, axis=0)
                    else:
                        importance = importance + np.sum(metric, axis=0)
                else:
                    if ii == 0:
                        metrics = metric
                    else:
                        metrics = np.concatenate((metrics, metric), axis=0)

                #
                # if epoch == 0:      # 只累计importance
                #     pass
                # else:
                #     dis[ii*self.batch_size: (ii+1)*self.batch_size] = np.linalg.norm(self.true_key_param-metric, axis=1) # L2范数
            self.net.train()

            if epoch == 0:
                self.param_musk = np.sort(importance.argsort()[:10000])     # 前10000个最重要的数据
                self.true_key_param = importance[self.param_musk] / len(self.train_data)
            else:
                importance = np.sum(metrics, axis=0) / len(self.train_data)
                self.true_key_param = importance
                dis = np.linalg.norm(importance - metrics, axis=1, ord=np.Inf)

                # metrics = np.insert(metrics, 0, self.train_label, axis=1)
                # df = pd.DataFrame(metrics)
                # df.to_csv('@g_detail/TcnED-DASADS-ICLR21-ori/%s.csv' % str(epoch))

                add_num = min(int(self.add_rate * len(self.train_data)), int(self.n_samples * 0.4))  # 每次添加的数据量
                index = dis.argsort()[:add_num]  # 扩展距离最小的40%
                index = np.sort(index)
                add_seq_starts = self.seq_starts[index]

                delet_num = min(int(self.del_rate * len(self.train_data)), int(self.n_samples * 0.2))  # 每次添加的数据量
                index = dis.argsort()[::-1][:delet_num]  # 删除距离最大的20%
                index = np.sort(index)
                self.seq_starts = np.delete(self.seq_starts, index, axis=0)

                for add_seq_start in add_seq_starts:
                    if add_seq_start - self.split[0] >= 0:
                        self.seq_starts = np.append(self.seq_starts, add_seq_start - self.split[0])
                    if add_seq_start + self.split[1] <= self.n_samples - self.seq_len + 1:
                        self.seq_starts = np.append(self.seq_starts, add_seq_start + self.split[1])
                    if add_seq_start - self.split[1] >= 0:
                        self.seq_starts = np.append(self.seq_starts, add_seq_start - self.split[1])
                    if add_seq_start + self.split[0] <= self.n_samples - self.seq_len + 1:
                        self.seq_starts = np.append(self.seq_starts, add_seq_start + self.split[0])

                self.seq_starts = np.sort(self.seq_starts)
                self.seq_starts = np.unique(self.seq_starts, axis=0)
                self.train_data = np.array([self.ori_data[i:i + self.seq_len] for i in self.seq_starts])  # 添加划分的数据
                self.train_loader = DataLoader(self.train_data, batch_size=self.batch_size, drop_last=False,
                                               shuffle=True, pin_memory=True)
                #
                y_seqs = get_sub_seqs_label2(self.ori_label, seq_starts=self.seq_starts,
                                             seq_len=self.seq_len) if self.ori_label is not None else None
                self.train_label = y_seqs
                self.trainsets['seqstarts' + str(epoch)] = self.seq_starts
                if y_seqs is not None:
                    self.trainsets['yseq' + str(epoch)] = y_seqs
                self.trainsets['dis' + str(epoch)] = dis

        elif self.sample_selection == 1:        # 我的方法 特别测试
            if len(self.train_data) <= int(self.n_samples * 0.5):
                return

            dis = np.zeros(len(self.train_data))
            value = np.zeros(len(self.train_data))
            num = np.zeros(len(self.train_data))
            importance = None
            self.net.eval()
            for ii, batch_x in enumerate(self.train_loader):
                # metric = self.get_importance_dL(batch_x, epoch, ii)
                metric = self.get_importance_ICLR21(batch_x, epoch, ii)
                # metric = self.get_importance_ICML17(batch_x, epoch, ii)       # 巨慢

                if ii == 0:
                    importance = np.sum(metric, axis=0)
                    thresh = importance[importance.argsort()[int(len(importance) * 0.4)]]/self.batch_size
                else:
                    importance = importance + np.sum(metric, axis=0)

                if epoch != 0:  # 非第一轮迭代，计算距离
                    dis[ii*self.batch_size: (ii+1)*self.batch_size] = np.linalg.norm(self.true_key_param-metric, axis=1, ord=np.Inf) # L2范数
                    # num[ii*self.batch_size: (ii+1)*self.batch_size] = metric
                value[ii*self.batch_size: (ii+1)*self.batch_size] = np.sum(metric, axis=1)
                num[ii*self.batch_size: (ii+1)*self.batch_size] = [sum(m > thresh) for m in metric]
            self.net.train()

            if epoch == 0:
                self.param_musk = np.sort(importance.argsort()[:10000])     # 前10000个最重要的数据
            else:
                # 待修改，根据重要参数，调整波动
                delet_num = min(int(self.add_rate * len(self.train_data)), int(self.n_samples * 0.2))        # 每次删除的数据量
                index = dis.argsort()[::-1][:delet_num]  # 删除距离最大的20%
                self.train_data = np.delete(self.train_data, index, axis=0)         # 删除指定行
                self.train_label = np.delete(self.train_label, index, axis=0)

                self.train_loader = DataLoader(self.train_data, batch_size=self.batch_size, drop_last=False,
                                               shuffle=True, pin_memory=True)

                self.trainsets['yseq' + str(epoch)] = self.train_label

            importance = importance[self.param_musk]
            self.true_key_param = importance/len(self.train_data)
            self.trainsets['dis' + str(epoch)] = dis
            self.trainsets['value' + str(epoch)] = value
            self.trainsets['num' + str(epoch)] = num
        else:
            print('ERROR')

    def get_importance_dL(self, batch_x, epoch, ii):
        _, losses = self.inference_forward(batch_x, self.net, self.criterion)
        gv_metric = []

        if epoch == 0 and ii == 0:             # 是第一个batch，更新all_v
            params = [p for p in self.net.parameters() if p.requires_grad]
            loss = losses[0]
            g_loss = grad(loss, params, create_graph=True, allow_unused=True)
            for jj, g in enumerate(g_loss):
                if g is not None:
                    self.params.append(params[jj])

        for jj, loss in enumerate(losses):
            # 有提速空间 #
            g_loss = grad(loss, self.params, create_graph=True, allow_unused=True)
            gv = torch.cat([g.view(-1) for g in g_loss]).cpu().detach().numpy()
            if self.param_musk is not None:
                gv = gv[self.param_musk]    # 只保留最重要的万个参数

            gv_metric.append(abs(gv))

        # mean = np.mean(gv_metric, axis=0)
        # # metric = metric / mean      # 按列归一化
        # gv_metric = np.divide(gv_metric, mean, out=np.zeros_like(gv_metric, dtype=np.float64), where=mean != 0)
        # gv_metric = normalize(gv_metric, axis=1, norm='l2')   # 对metric按行进行归一化
        return gv_metric

    def get_importance_ICLR21(self, batch_x, epoch, ii):
        _, losses = self.inference_forward(batch_x, self.net, self.criterion)

        if ii == 0:             # 是第一个batch，更新all_v
            if epoch != 0:      # 不是第一轮迭代
                self.all_v = np.concatenate([param.data.view(-1).cpu().detach().numpy() for param in self.params])[
                    self.param_musk]
            else:
                params = [p for p in self.net.parameters() if p.requires_grad]
                loss = losses[0]
                g_loss = grad(loss, params, create_graph=True, allow_unused=True)
                for jj, g in enumerate(g_loss):
                    if g is not None:
                        self.params.append(params[jj])
                self.all_v = torch.cat([param.data.view(-1) for param in self.params]).cpu().detach().numpy()

        gv_metric = []
        for jj, loss in enumerate(losses):
            # 有提速空间 #
            g_loss = grad(loss, self.params, create_graph=True, allow_unused=True)
            gv = torch.cat([g.view(-1) for g in g_loss]).cpu().detach().numpy()
            if self.param_musk is not None:
                gv = gv[self.param_musk]    # 只保留最重要的万个参数

            gv_metric.append(gv)
        metric = abs(gv_metric * self.all_v)
        mean = np.mean(metric, axis=0)
        # metric = metric / mean      # 按列归一化
        metric = np.divide(metric, mean, out=np.zeros_like(metric, dtype=np.float64), where=mean != 0)
        metric = normalize(metric, axis=1, norm='l2')   # 对metric按行进行归一化
        return metric

    def get_importance_ICML17(self, batch_x, epoch, ii):
        _, losses = self.inference_forward(batch_x, self.net, self.criterion)
        if epoch == 0 and ii == 0:
            params = [p for p in self.net.parameters() if p.requires_grad]
            loss = losses[0]
            g_loss = grad(loss, params, create_graph=True, allow_unused=True)
            for jj, g in enumerate(g_loss):
                if g is not None:
                    self.params.append(params[jj])

        size = len(batch_x)
        metric = []
        for z_train in batch_x:
            h_estimate = self.s_test(z_train, batch_x)
            h_estimate = torch.cat([h.view(-1) for h in h_estimate]).cpu().detach().numpy()
            metric.append(h_estimate)

        metric = np.array(metric)
        metric = np.sum(metric, axis=0)/size        # 按列求和

        #
        # gv_metric1 = []
        # gv_metric2 = []
        # for jj, loss in enumerate(losses):
        #     # 有提速空间 #
        #     g_loss1 = grad(loss, self.params, create_graph=True, allow_unused=True)         # 1阶导
        #     g_loss2 = hessian(loss, self.params)        # 有问题
        #     gv1 = torch.cat([g.view(-1) for g in g_loss1]).cpu().detach().numpy()
        #     gv2 = torch.cat([g.view(-1) for g in g_loss2]).cpu().detach().numpy()
        #     if self.param_musk is not None:
        #         gv1 = gv1[self.param_musk]  # 只保留最重要的万个参数
        #         gv2 = gv2[self.param_musk, :]  # 只保留最重要的万个参数
        #         gv2 = gv2[:, self.param_musk]  # 只保留最重要的万个参数，矩阵
        #     gv_metric1.append(gv1)
        #     if jj == 0:
        #         gv_metric2 = gv2
        #     else:
        #         gv_metric2 += gv2

        # H = np.linalg.inv(gv_metric2)
        # gv_metric1 = np.array(gv_metric1)
        # metric = np.dot(H, gv_metric1) / size
        return metric

    def s_test(self, z_train, batch_x, damp=0.01, scale=25.0,
               recursion_depth=5000):
        self.net.eval()
        z_train = self.train_loader.collate_fn([z_train])
        _, loss = self.inference_forward(z_train, self.net, self.criterion)
        v = list(grad(loss, self.params, create_graph=True))
        h_estimate = v.copy()
        for i in range(recursion_depth):
            _, losses = self.inference_forward(batch_x, self.net, self.criterion)
            loc = np.random.randint(len(batch_x))
            loss = losses[loc]
            hv = self.hvp(loss, self.params, h_estimate)
            # Recursively caclulate h_estimate  递归计算 求逆
            h_estimate = [
                _v + (1 - damp) * _h_e - _hv / scale
                for _v, _h_e, _hv in zip(v, h_estimate, hv)]
        return h_estimate

    def s_test_old(self, z_train, batch_x, damp=0.01, scale=25.0,
               recursion_depth=5000):
        """s_test can be precomputed for each test point of interest, and then
        multiplied with grad_z to get the desired value for each training point.
        Here, strochastic estimation is used to calculate s_test. s_test is the
        Inverse Hessian Vector Product.

        Arguments:
            z_test: torch tensor, test data points, such as test images
            t_test: torch tensor, contains all test data labels
            model: torch NN, model used to evaluate the dataset
            z_loader: torch Dataloader, can load the training dataset
            gpu: int, GPU id to use if >=0 and -1 means use CPU
            damp: float, dampening factor
            scale: float, scaling factor
            recursion_depth: int, number of iterations aka recursion depth
                should be enough so that the value stabilises.

        Returns:
            h_estimate: list of torch tensors, s_test"""

        self.net.eval()
        z_train = self.train_loader.collate_fn([z_train])
        _, loss = self.inference_forward(z_train, self.net, self.criterion)
        v = list(grad(loss, self.params, create_graph=True))
        h_estimate = v.copy()
        ################################
        # once h_estimate stabilises
        ################################
        for i in range(recursion_depth):
            # take just one random sample from training dataset
            # easiest way to just use the DataLoader once, break at the end of loop
            #########################
            _, losses = self.inference_forward(batch_x, self.net, self.criterion)
            loc = np.random.randint(len(batch_x))
            loss = losses[loc]
            # loss = self.training_forward(batch_x, self.net, self.criterion)
            hv = self.hvp(loss, self.params, h_estimate)
            # Recursively caclulate h_estimate  递归计算 求逆
            h_estimate = [
                _v + (1 - damp) * _h_e - _hv / scale
                for _v, _h_e, _hv in zip(v, h_estimate, hv)]
        return h_estimate

    def hvp(self, y, w, v):
        """
        Arguments:
            y: 标量/tensor，通常来说为loss函数的输出
            w: pytorch tensor的list，代表需要被求解hessian矩阵的参数
            v: pytorch tensor的list，代表需要与hessian矩阵乘积的向量
        Returns:
            return_grads: pytorch tensor的list, 代表hvp的最终结果.
        Raises:
            ValueError: y 与 w 长度不同."""
        if len(w) != len(v):
            raise (ValueError("w and v must have the same length."))
        # First backprop
        first_grads = grad(y, w, retain_graph=True, create_graph=True)
        # Second backprop
        return_grads = grad(first_grads, w, grad_outputs=v, retain_graph=False, create_graph=False)
        return return_grads
